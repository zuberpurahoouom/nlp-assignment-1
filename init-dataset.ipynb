{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff33c4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File qidpidtriples.train.full.2.tsv.gz already exists.\n",
      "File qidpidtriples.train.full.2.tsv already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import gzip, shutil\n",
    "import tarfile, io\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "files = [\n",
    "    {\n",
    "        \"url\": \"https://msmarco.z22.web.core.windows.net/msmarcoranking/qidpidtriples.train.full.2.tsv.gz\",\n",
    "        \"name\": \"qidpidtriples.train.full.2.tsv.gz\",\n",
    "        \"decompressed\": \"qidpidtriples.train.full.2.tsv\"\n",
    "    },\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    if os.path.exists(file[\"decompressed\"]) or os.path.exists(file[\"name\"]):\n",
    "        print(f\"File {file['name']} already exists.\")\n",
    "    else:\n",
    "        print(\"Downloading\")\n",
    "        response = requests.get(file[\"url\"])\n",
    "        with open(file[\"name\"], \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    decompressed = file[\"decompressed\"]\n",
    "    filename = file[\"name\"]\n",
    "    if not os.path.exists(decompressed) and filename.endswith('.tar.gz'):\n",
    "        with tarfile.open(file[\"name\"], 'r:gz') as tar:\n",
    "            print(\"Decompressing tar.gz file...\")\n",
    "            tar.extractall(path='.')    \n",
    "    elif not os.path.exists(decompressed) and filename.endswith(\".gz\"):\n",
    "        with gzip.open(file[\"name\"], \"rb\") as src, open(decompressed, \"wb\") as dst:\n",
    "            print(\"Decompressing .gz file...\")\n",
    "            shutil.copyfileobj(src, dst)\n",
    "            print(f\"Decompressed to {file['decompressed']}\")\n",
    "    else:\n",
    "        print(f\"File {decompressed} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdec4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "qrels_dev = \"qrels.dev.tsv\"\n",
    "collection_tsv = \"collection.tsv\"\n",
    "\n",
    "# Read qrels (judged passages)\n",
    "qrels = pd.read_csv(\n",
    "    qrels_dev,\n",
    "    sep=\"\\t\",\n",
    "    names=[\"qid\", \"_\", \"pid\", \"rel\"],\n",
    "    dtype={\"qid\": str, \"pid\": str, \"rel\": int}\n",
    ")\n",
    "\n",
    "judged = set(qrels[\"pid\"])\n",
    "\n",
    "out_tsv = \"common_dataset.tsv\"\n",
    "\n",
    "# total size of out_tsv will be greater than this as you will see in below cell\n",
    "target_size = 20_000\n",
    "\n",
    "if not os.path.exists(out_tsv):\n",
    "    written = set()\n",
    "    limit = min(target_size, len(judged))  # only take up to target_size judged passages\n",
    "    with open(out_tsv, \"w\", encoding=\"utf-8\") as out:\n",
    "        for chunk in pd.read_csv(\n",
    "            collection_tsv,\n",
    "            sep=\"\\t\",\n",
    "            names=[\"pid\", \"text\"],\n",
    "            dtype={\"pid\": str, \"text\": str},\n",
    "            chunksize=1_000_000,\n",
    "            quoting=3,\n",
    "            on_bad_lines=\"skip\"\n",
    "        ):\n",
    "            if len(written) >= limit:\n",
    "                break\n",
    "            # Filter judged not yet written\n",
    "            remaining = limit - len(written)\n",
    "            keep = chunk[chunk[\"pid\"].isin(judged - written)]\n",
    "            if keep.empty:\n",
    "                continue\n",
    "            if len(keep) > remaining:\n",
    "                keep = keep.iloc[:remaining]\n",
    "            keep.to_csv(out, sep=\"\\t\", header=False, index=False)\n",
    "            written.update(keep[\"pid\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3577138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n"
     ]
    }
   ],
   "source": [
    "common_dataset = pd.read_csv(out_tsv, sep=\"\\t\", names=[\"pid\", \"text\"], dtype={\"pid\": str, \"text\": str})\n",
    "print(common_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06e7f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 30_000\n",
    "seed=42\n",
    "# add random distractors to reach target_size\n",
    "need = max(0, target_size - len(written))\n",
    "if need > 0:\n",
    "        for chunk in pd.read_csv(collection_tsv, sep=\"\\t\",\n",
    "                                 names=[\"pid\",\"text\"], dtype={\"pid\":str,\"text\":str},\n",
    "                                 chunksize=1_000_000, quoting=3, on_bad_lines=\"skip\"):\n",
    "            cand = chunk[~chunk[\"pid\"].isin(written)]\n",
    "            if len(cand) == 0: \n",
    "                continue\n",
    "            take = min(need, len(cand))\n",
    "            samp = cand.sample(n=take, random_state=seed)\n",
    "            with open(out_tsv, \"a\", encoding=\"utf-8\") as out:\n",
    "                samp.to_csv(out, sep=\"\\t\", header=False, index=False)\n",
    "            written.update(samp[\"pid\"])\n",
    "            need -= take\n",
    "            if need == 0:\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf922d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 2)\n"
     ]
    }
   ],
   "source": [
    "common_dataset = pd.read_csv(out_tsv, sep=\"\\t\", names=[\"pid\", \"text\"], dtype={\"pid\": str, \"text\": str})\n",
    "print(common_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# Build merged_full incrementally up to target_size with progress bars\n",
    "pid_text = common_dataset.set_index(\"pid\")[\"text\"]\n",
    "filename = \"qidpidtriples.train.full.2.tsv\"\n",
    "\n",
    "parts = []\n",
    "total = 0\n",
    "\n",
    "reader = pd.read_csv(\n",
    "    filename,\n",
    "    sep=\"\\t\",\n",
    "    names=[\"qid\", \"pos_pid\", \"neg_pid\"],\n",
    "    dtype={\"qid\": str, \"pos_pid\": str, \"neg_pid\": str},\n",
    "    chunksize=500_000\n",
    ")\n",
    "\n",
    "for triples_chunk in tqdm(reader, desc=\"Reading chunks\"):\n",
    "    mask = triples_chunk[\"pos_pid\"].isin(pid_text.index) & triples_chunk[\"neg_pid\"].isin(pid_text.index)\n",
    "    if not mask.any():\n",
    "        continue\n",
    "    sub = triples_chunk.loc[mask].copy()\n",
    "    sub[\"pos_text\"] = sub[\"pos_pid\"].map(pid_text)\n",
    "    sub[\"neg_text\"] = sub[\"neg_pid\"].map(pid_text)\n",
    "    parts.append(sub)\n",
    "    gained = len(sub)\n",
    "    total += gained\n",
    "\n",
    "merged_full = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "print(merged_full.shape)\n",
    "print(merged_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50228435",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_full.to_csv(\"merged_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd71782f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max passage length (tokens): 233\n",
      "           pid  token_len                                               text\n",
      "28548   564040        233  The subshell with the quantum numbers n =4, l ...\n",
      "22311   123946        204  full time undergraduate students that do not r...\n",
      "9190   7086125        198  Order Online at the #1 Source for Gourmet Food...\n",
      "13295  7147363        197  The DNA double helix is held together by two t...\n",
      "21840   357224        196  We use cookies to enhance your experience on o...\n",
      "15927  7186716        194  An asphalt apron typically costs $2-$5 a squar...\n",
      "10199  7100389        189  A long-term goal is something you want to do i...\n",
      "15936  7186847        189  Predetermined overhead rate is estimated overh...\n",
      "10192  7100293        187  Last week in one of our afkar.me, our incubati...\n",
      "17703  7214400        186  Butterfly Needles Explained A butterfly needle...\n"
     ]
    }
   ],
   "source": [
    "# Compute passage lengths (in tokens) using a simple word regex and display max + top 10 longest\n",
    "if \"token_len\" not in common_dataset.columns:\n",
    "    common_dataset[\"token_len\"] = common_dataset[\"text\"].str.findall(r\"\\w+\").str.len()\n",
    "\n",
    "max_tokens = common_dataset[\"token_len\"].max()\n",
    "print(\"Max passage length (tokens):\", max_tokens)\n",
    "\n",
    "top10_tokens = common_dataset.nlargest(10, \"token_len\")[[\"pid\", \"token_len\", \"text\"]]\n",
    "print(top10_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde8106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
