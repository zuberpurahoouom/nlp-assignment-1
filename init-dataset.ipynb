{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff33c4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "File qidpidtriples.train.full.2.tsv.gz already exists.\n",
      "File qidpidtriples.train.full.2.tsv already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import gzip, shutil\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "files = [\n",
    "    {\n",
    "        \"url\": \"https://msmarco.z22.web.core.windows.net/msmarcoranking/qidpidtriples.train.full.2.tsv.gz\",\n",
    "        \"name\": \"qidpidtriples.train.full.2.tsv.gz\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    if os.path.exists(file[\"name\"].replace(\".gz\", \"\")):\n",
    "        print(f\"File {file['name']} already exists.\")\n",
    "    else:\n",
    "        response = requests.get(file[\"url\"])\n",
    "        with open(file[\"name\"], \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    filename = file[\"name\"].replace(\".gz\", \"\")\n",
    "    if not os.path.exists(filename):\n",
    "        with gzip.open(file[\"name\"], \"rb\") as src, open(filename, \"wb\") as dst:\n",
    "            shutil.copyfileobj(src, dst)\n",
    "            print(f\"Decompressed to {filename}\")\n",
    "    else:\n",
    "        print(f\"File {filename} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "qrels_dev = \"qrels.dev.tsv\"\n",
    "collection_tsv = \"collection.tsv\"\n",
    "\n",
    "# Read qrels (judged passages)\n",
    "qrels = pd.read_csv(\n",
    "    qrels_dev,\n",
    "    sep=\"\\t\",\n",
    "    names=[\"qid\", \"_\", \"pid\", \"rel\"],\n",
    "    dtype={\"qid\": str, \"pid\": str, \"rel\": int}\n",
    ")\n",
    "\n",
    "judged = set(qrels[\"pid\"])\n",
    "\n",
    "out_tsv = \"common_dataset.tsv\"\n",
    "\n",
    "# total size of out_tsv will be greater than this as you will see in below cell\n",
    "target_size = 20_000\n",
    "\n",
    "if not os.path.exists(out_tsv):\n",
    "    written = set()\n",
    "    limit = min(target_size, len(judged))  # only take up to target_size judged passages\n",
    "    with open(out_tsv, \"w\", encoding=\"utf-8\") as out:\n",
    "        for chunk in pd.read_csv(\n",
    "            collection_tsv,\n",
    "            sep=\"\\t\",\n",
    "            names=[\"pid\", \"text\"],\n",
    "            dtype={\"pid\": str, \"text\": str},\n",
    "            chunksize=1_000_000,\n",
    "            quoting=3,\n",
    "            on_bad_lines=\"skip\"\n",
    "        ):\n",
    "            if len(written) >= limit:\n",
    "                break\n",
    "            # Filter judged not yet written\n",
    "            remaining = limit - len(written)\n",
    "            keep = chunk[chunk[\"pid\"].isin(judged - written)]\n",
    "            if keep.empty:\n",
    "                continue\n",
    "            if len(keep) > remaining:\n",
    "                keep = keep.iloc[:remaining]\n",
    "            keep.to_csv(out, sep=\"\\t\", header=False, index=False)\n",
    "            written.update(keep[\"pid\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3577138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n"
     ]
    }
   ],
   "source": [
    "common_dataset = pd.read_csv(out_tsv, sep=\"\\t\", names=[\"pid\", \"text\"], dtype={\"pid\": str, \"text\": str})\n",
    "print(common_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06e7f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 40_000\n",
    "seed=42\n",
    "# add random distractors to reach target_size\n",
    "need = max(0, target_size - len(written))\n",
    "if need > 0:\n",
    "        for chunk in pd.read_csv(collection_tsv, sep=\"\\t\",\n",
    "                                 names=[\"pid\",\"text\"], dtype={\"pid\":str,\"text\":str},\n",
    "                                 chunksize=1_000_000, quoting=3, on_bad_lines=\"skip\"):\n",
    "            cand = chunk[~chunk[\"pid\"].isin(written)]\n",
    "            if len(cand) == 0: \n",
    "                continue\n",
    "            take = min(need, len(cand))\n",
    "            samp = cand.sample(n=take, random_state=seed)\n",
    "            with open(out_tsv, \"a\", encoding=\"utf-8\") as out:\n",
    "                samp.to_csv(out, sep=\"\\t\", header=False, index=False)\n",
    "            written.update(samp[\"pid\"])\n",
    "            need -= take\n",
    "            if need == 0:\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf922d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2)\n"
     ]
    }
   ],
   "source": [
    "common_dataset = pd.read_csv(out_tsv, sep=\"\\t\", names=[\"pid\", \"text\"], dtype={\"pid\": str, \"text\": str})\n",
    "print(common_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c9543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading chunks: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading chunks: 40it [00:16,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1000/1000 (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading chunks: 796it [05:05,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target reached.\n",
      "(18691, 5)\n",
      "       qid  pos_pid  neg_pid  \\\n",
      "0  1150887  1504586  7669864   \n",
      "1  1150887  1504586  7622515   \n",
      "2  1150887  1504586  7684386   \n",
      "3  1150887  1504586  7672621   \n",
      "4   242510  6054949  7329242   \n",
      "\n",
      "                                            pos_text  \\\n",
      "0  The NVL function. You can use the NVL function...   \n",
      "1  The NVL function. You can use the NVL function...   \n",
      "2  The NVL function. You can use the NVL function...   \n",
      "3  The NVL function. You can use the NVL function...   \n",
      "4  Seagulls live anywhere between 5 to 15 years d...   \n",
      "\n",
      "                                            neg_text  \n",
      "0  The Excel Len function is a very useful functi...  \n",
      "1  In math, inverse is explained as the opposite ...  \n",
      "2  Method is from the object oriented paradigm an...  \n",
      "3  Recursive function. Recursive function, in log...  \n",
      "4  Answer by soundoff (121) Pit Bulls live betwee...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# Build merged_full incrementally up to target_size with progress bars\n",
    "pid_text = common_dataset.set_index(\"pid\")[\"text\"]\n",
    "filename = \"qidpidtriples.train.full.2.tsv\"\n",
    "\n",
    "parts = []\n",
    "total = 0\n",
    "\n",
    "reader = pd.read_csv(\n",
    "    filename,\n",
    "    sep=\"\\t\",\n",
    "    names=[\"qid\", \"pos_pid\", \"neg_pid\"],\n",
    "    dtype={\"qid\": str, \"pos_pid\": str, \"neg_pid\": str},\n",
    "    chunksize=500_000\n",
    ")\n",
    "\n",
    "for triples_chunk in tqdm(reader, desc=\"Reading chunks\"):\n",
    "    mask = triples_chunk[\"pos_pid\"].isin(pid_text.index) & triples_chunk[\"neg_pid\"].isin(pid_text.index)\n",
    "    if not mask.any():\n",
    "        continue\n",
    "    sub = triples_chunk.loc[mask].copy()\n",
    "    sub[\"pos_text\"] = sub[\"pos_pid\"].map(pid_text)\n",
    "    sub[\"neg_text\"] = sub[\"neg_pid\"].map(pid_text)\n",
    "    parts.append(sub)\n",
    "    gained = len(sub)\n",
    "    total += gained\n",
    "\n",
    "merged_full = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "print(merged_full.shape)\n",
    "print(merged_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50228435",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_full.to_csv(\"merged_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c802d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
